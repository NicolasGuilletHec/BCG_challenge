{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Imports & paths\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "YIELD_PATH = \"../data/bronze/barley_yield_from_1982.csv\"\n",
    "CLIMATE_PATH = \"../data/bronze/climate_data_from_1982.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Load correctly (IMPORTANT)\n",
    "# =========================\n",
    "# Your CSV is ; separated -> sep=\";\"\n",
    "df_yield = pd.read_csv(YIELD_PATH, sep=\";\")\n",
    "\n",
    "df_climate = pd.read_parquet(CLIMATE_PATH)\n",
    "\n",
    "print(\"df_yield:\", df_yield.shape)\n",
    "print(\"df_climate:\", df_climate.shape)\n",
    "\n",
    "display(df_yield.head())\n",
    "display(df_climate.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Utility functions (EDA helpers)\n",
    "# =========================\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean column names.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "        .str.replace(r\"[^a-z0-9_]\", \"\", regex=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def overview(df: pd.DataFrame, name: str):\n",
    "    \"\"\"Print an overview of the DataFrame.\"\"\"\n",
    "    print(f\"\\n{'=' * 90}\\n{name}\\n{'=' * 90}\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    display(df.head(5))\n",
    "\n",
    "    print(\"\\n--- dtypes ---\")\n",
    "    # FIX: make dtypes sortable by converting to string\n",
    "    dtypes_sorted = df.dtypes.astype(str).sort_values()\n",
    "    display(dtypes_sorted.to_frame(\"dtype\"))\n",
    "\n",
    "    print(\"\\n--- missing (% top 25) ---\")\n",
    "    miss = (df.isna().mean().sort_values(ascending=False) * 100).head(25)\n",
    "    display(miss.to_frame(\"% missing\"))\n",
    "\n",
    "    print(\"\\n--- duplicates ---\")\n",
    "    print(\"duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "\n",
    "def numeric_stats(df: pd.DataFrame, name: str):\n",
    "    \"\"\"Calculate and display statistics for numeric columns.\"\"\"\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    print(f\"\\n{name} - numeric columns:\", list(num.columns))\n",
    "    if num.shape[1] == 0:\n",
    "        return\n",
    "    display(num.describe().T)\n",
    "\n",
    "\n",
    "def categorical_stats(df: pd.DataFrame, name: str, topk=15, max_cols=8):\n",
    "    \"\"\"Calculate and display statistics for categorical columns.\"\"\"\n",
    "    cat_cols = df.select_dtypes(\n",
    "        include=[\"object\", \"string\", \"category\", \"bool\"]\n",
    "    ).columns.tolist()\n",
    "    print(f\"\\n{name} - categorical columns:\", cat_cols)\n",
    "    if not cat_cols:\n",
    "        return\n",
    "    card = df[cat_cols].nunique(dropna=True).sort_values(ascending=False)\n",
    "    display(card.to_frame(\"n_unique\"))\n",
    "\n",
    "    for c in card.head(min(max_cols, len(card))).index:\n",
    "        print(f\"\\nTop values for '{c}'\")\n",
    "        display(df[c].value_counts(dropna=False).head(topk).to_frame(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Clean column names + basic overview\n",
    "# =========================\n",
    "df_yield = clean_cols(df_yield)\n",
    "df_climate = clean_cols(df_climate)\n",
    "\n",
    "overview(df_yield, \"df_yield\")\n",
    "overview(df_climate, \"df_climate\")\n",
    "\n",
    "numeric_stats(df_yield, \"df_yield\")\n",
    "categorical_stats(df_yield, \"df_yield\")\n",
    "\n",
    "# df_climate is huge: we do careful summaries below\n",
    "categorical_stats(df_climate, \"df_climate\", topk=10, max_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) df_yield checks\n",
    "# =========================\n",
    "# Common expected columns given your screenshot: department, year, yield, area,\n",
    "# production\n",
    "print(df_yield.columns)\n",
    "\n",
    "# Convert numeric columns safely if needed\n",
    "for c in [\"year\", \"yield\", \"area\", \"production\"]:\n",
    "    if c in df_yield.columns:\n",
    "        df_yield[c] = pd.to_numeric(df_yield[c], errors=\"coerce\")\n",
    "\n",
    "# Basic sanity checks\n",
    "if \"year\" in df_yield.columns:\n",
    "    print(\"Year range:\", df_yield[\"year\"].min(), \"->\", df_yield[\"year\"].max())\n",
    "\n",
    "# Quick plots\n",
    "if \"yield\" in df_yield.columns:\n",
    "    plt.figure()\n",
    "    plt.hist(df_yield[\"yield\"].dropna(), bins=40)\n",
    "    plt.title(\"Yield distribution\")\n",
    "    plt.xlabel(\"yield\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "# If you have a department column, show top departments by avg yield\n",
    "dep_col = None\n",
    "for c in [\"department\", \"nom_dep\", \"dep\", \"departement\"]:\n",
    "    if c in df_yield.columns:\n",
    "        dep_col = c\n",
    "        break\n",
    "\n",
    "if dep_col and \"yield\" in df_yield.columns:\n",
    "    dep_summary = (\n",
    "        df_yield.groupby(dep_col)[\"yield\"]\n",
    "        .agg(count=\"count\", mean=\"mean\", median=\"median\")\n",
    "        .sort_values(\"mean\", ascending=False)\n",
    "    )\n",
    "    display(dep_summary.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) df_climate structure checks\n",
    "# =========================\n",
    "print(df_climate.columns)\n",
    "\n",
    "# Parse time if needed\n",
    "if \"time\" in df_climate.columns and not np.issubdtype(\n",
    "    df_climate[\"time\"].dtype, np.datetime64\n",
    "):\n",
    "    df_climate[\"time\"] = pd.to_datetime(df_climate[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Key distributions (fast-ish even on large data)\n",
    "for c in [\"scenario\", \"year\", \"metric\"]:\n",
    "    if c in df_climate.columns:\n",
    "        print(f\"\\nValue counts for {c} (top 20):\")\n",
    "        display(df_climate[c].value_counts(dropna=False).head(20).to_frame(\"count\"))\n",
    "\n",
    "# Date range\n",
    "if \"time\" in df_climate.columns:\n",
    "    print(\"\\nTime range:\", df_climate[\"time\"].min(), \"->\", df_climate[\"time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6) Missingness & numeric stats\n",
    "# =========================\n",
    "# Full missingness (still ok)\n",
    "miss_cl = df_climate.isna().mean().sort_values(ascending=False) * 100\n",
    "display(miss_cl.head(20).to_frame(\"% missing\"))\n",
    "\n",
    "# Numeric describe on full data (usually ok)\n",
    "numeric_stats(df_climate, \"df_climate\")\n",
    "\n",
    "# For plots: sample (so it's responsive)\n",
    "SAMPLE_N = 300_000  # adjust if too heavy/too light\n",
    "df_climate_s = df_climate.sample(n=min(SAMPLE_N, len(df_climate)), random_state=42)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df_climate_s[\"value\"].dropna(), bins=60)\n",
    "plt.title(\"Climate 'value' distribution (sample)\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) Yearly aggregation (key step)\n",
    "# =========================\n",
    "group_cols = [\n",
    "    c\n",
    "    for c in [\"scenario\", \"code_dep\", \"nom_dep\", \"year\", \"metric\"]\n",
    "    if c in df_climate.columns\n",
    "]\n",
    "print(\"Grouping on:\", group_cols)\n",
    "\n",
    "climate_yearly = df_climate.groupby(group_cols, as_index=False)[\"value\"].agg(\n",
    "    n=\"count\",\n",
    "    mean=\"mean\",\n",
    "    std=\"std\",\n",
    "    min=\"min\",\n",
    "    p05=lambda x: x.quantile(0.05),\n",
    "    p50=lambda x: x.quantile(0.50),\n",
    "    p95=lambda x: x.quantile(0.95),\n",
    "    max=\"max\",\n",
    ")\n",
    "\n",
    "print(\"climate_yearly:\", climate_yearly.shape)\n",
    "display(climate_yearly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8) Plot: yearly mean by scenario for a chosen metric\n",
    "# =========================\n",
    "# Pick one metric automatically (most frequent)\n",
    "metric_choice = (\n",
    "    df_climate[\"metric\"].value_counts().index[0]\n",
    "    if \"metric\" in df_climate.columns\n",
    "    else None\n",
    ")\n",
    "print(\"Chosen metric:\", metric_choice)\n",
    "\n",
    "if metric_choice:\n",
    "    tmp = climate_yearly[climate_yearly[\"metric\"] == metric_choice].copy()\n",
    "\n",
    "    # choose up to 3 scenarios to plot\n",
    "    if \"scenario\" in tmp.columns:\n",
    "        scenarios = tmp[\"scenario\"].dropna().unique()[:3]\n",
    "    else:\n",
    "        scenarios = [None]\n",
    "\n",
    "    for sc in scenarios:\n",
    "        plot_df = tmp if sc is None else tmp[tmp[\"scenario\"] == sc]\n",
    "\n",
    "        # aggregate across departments to get a national-ish curve\n",
    "        national = plot_df.groupby(\"year\", as_index=False)[\"mean\"].mean()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(national[\"year\"], national[\"mean\"])\n",
    "        plt.title(\n",
    "            f\"Mean yearly {metric_choice} (avg across deps)\"\n",
    "            + (\"\" if sc is None else f\" — {sc}\")\n",
    "        )\n",
    "        plt.xlabel(\"year\")\n",
    "        plt.ylabel(\"mean(value)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- France department choropleths (production / yield) ---\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "GEOJSON_URL = \"https://france-geojson.gregoiredavid.fr/repo/departements.geojson\"\n",
    "CACHE_DIR = \"./_geo_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "GEOJSON_PATH = os.path.join(CACHE_DIR, \"departements.geojson\")\n",
    "\n",
    "\n",
    "def load_departments_geometries() -> gpd.GeoDataFrame:\n",
    "    \"\"\"Load department geometries from geojson.\"\"\"\n",
    "    if not os.path.exists(GEOJSON_PATH):\n",
    "        r = requests.get(GEOJSON_URL, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with open(GEOJSON_PATH, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    gdf = gpd.read_file(GEOJSON_PATH)\n",
    "    gdf[\"nom\"] = gdf[\"nom\"].astype(str)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        ch for ch in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(ch)\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_dep_name(x) -> str:\n",
    "    \"\"\"Normalize department names so df_yield.department matches geojson.nom.\n",
    "\n",
    "    Handles underscores, hyphens, accents, apostrophes, extra spaces, casing.\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip().lower()\n",
    "    s = _strip_accents(s)\n",
    "\n",
    "    # unify punctuation\n",
    "    s = s.replace(\"’\", \"'\")\n",
    "    s = s.replace(\"_\", \" \")\n",
    "    s = s.replace(\"-\", \" \")\n",
    "\n",
    "    # collapse spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # some common variations\n",
    "    fixes = {\n",
    "        \"cotes d armor\": \"cotes d'armor\",\n",
    "        \"cote d or\": \"cote d'or\",\n",
    "        \"corse du sud\": \"corse-du-sud\",  # geojson uses hyphenated official spelling\n",
    "        \"haute corse\": \"haute-corse\",\n",
    "    }\n",
    "    return fixes.get(s, s)\n",
    "\n",
    "\n",
    "def plot_france_dep_choropleths(\n",
    "    df: pd.DataFrame,\n",
    "    year: int,\n",
    "    dep_name_col: str = \"department\",\n",
    "    year_col: str = \"year\",\n",
    "    production_col: str = \"production\",\n",
    "    yield_col: str = \"yield\",\n",
    "    agg: str = \"mean\",  # 'mean' or 'sum'\n",
    "    cmap: str = \"viridis\",\n",
    "    missing_color: str = \"lightgrey\",\n",
    "    figsize=(14, 7),\n",
    "):\n",
    "    \"\"\"Plot map of production and/or yield by French department for a given year.\"\"\"\n",
    "    gdf_deps = load_departments_geometries()\n",
    "\n",
    "    d = df.copy()\n",
    "    if dep_name_col not in d.columns:\n",
    "        raise ValueError(f\"Missing column '{dep_name_col}' in df.\")\n",
    "    if year_col not in d.columns:\n",
    "        raise ValueError(f\"Missing column '{year_col}' in df.\")\n",
    "\n",
    "    d[\"_dep_name_norm\"] = d[dep_name_col].apply(normalize_dep_name)\n",
    "    d[year_col] = pd.to_numeric(d[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    value_cols = [c for c in [production_col, yield_col] if c in d.columns]\n",
    "    if not value_cols:\n",
    "        raise ValueError(\n",
    "            f\"None of these columns found in df: {production_col}, {yield_col}\"\n",
    "        )\n",
    "\n",
    "    # Aggregate to dep-year\n",
    "    if agg == \"mean\":\n",
    "        d = d.groupby([\"_dep_name_norm\", year_col], as_index=False)[value_cols].mean()\n",
    "    elif agg == \"sum\":\n",
    "        d = d.groupby([\"_dep_name_norm\", year_col], as_index=False)[value_cols].sum()\n",
    "    else:\n",
    "        raise ValueError(\"agg must be 'mean' or 'sum'\")\n",
    "\n",
    "    d = d.dropna(subset=[\"_dep_name_norm\", year_col])\n",
    "    d[year_col] = d[year_col].astype(int)\n",
    "\n",
    "    # Normalize geojson names too\n",
    "    gdf_deps[\"_dep_name_norm\"] = gdf_deps[\"nom\"].apply(normalize_dep_name)\n",
    "\n",
    "    d_year = d[d[year_col] == int(year)]\n",
    "    g = gdf_deps.merge(d_year, how=\"left\", on=\"_dep_name_norm\")\n",
    "\n",
    "    plots = []\n",
    "    if production_col in g.columns:\n",
    "        plots.append((production_col, \"Production\"))\n",
    "    if yield_col in g.columns:\n",
    "        plots.append((yield_col, \"Yield\"))\n",
    "    if not plots:\n",
    "        raise ValueError(\n",
    "            \"After merge, production/yield columns not present. \"\n",
    "            \"Check your column names.\"\n",
    "        )\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(plots), figsize=figsize)\n",
    "    if len(plots) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (col, title) in zip(axes, plots, strict=False):\n",
    "        ax.set_title(f\"{title} — {year}\", fontsize=14)\n",
    "        ax.axis(\"off\")\n",
    "        g.plot(\n",
    "            column=col,\n",
    "            ax=ax,\n",
    "            legend=True,\n",
    "            cmap=cmap,\n",
    "            missing_kwds={\"color\": missing_color, \"label\": \"Missing\"},\n",
    "            linewidth=0.2,\n",
    "            edgecolor=\"white\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    matched = g[value_cols[0]].notna().sum()  # proxy\n",
    "    print(f\"Coverage: {matched}/{len(g)} departments matched for year {year}.\")\n",
    "\n",
    "\n",
    "# --- USAGE ---\n",
    "# plot_france_dep_choropleths(df_yield, year=2019)  # adjust year_col if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_france_dep_choropleths(df_yield, year=2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "24-data-strat-bcgx (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
