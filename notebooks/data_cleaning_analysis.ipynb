{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Missing Data Analysis\n",
    "\n",
    "**Goal**: Deep-dive into both datasets (yield & climate) to understand all data quality issues before deciding how to handle them.\n",
    "\n",
    "**Outline**:\n",
    "1. Load both raw datasets\n",
    "2. **Yield table** — missing values per column, per department, per year; recoverability; outliers; consistency checks\n",
    "3. **Climate table** — missing values, coverage gaps, data quality (negative precip), completeness per department/year\n",
    "4. **Department mismatch** — detailed comparison of department names between the two tables\n",
    "5. **Cross-table join analysis** — what happens when we merge; which rows gain NaN and why\n",
    "6. **Summary of all issues** — table of decisions to make together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load both raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "\n",
    "YIELD_PATH = \"../raw_data/barley_yield_from_1982.csv\"\n",
    "CLIMATE_PATH = \"../raw_data/climate_data_from_1982.parquet\"\n",
    "\n",
    "# Load\n",
    "df_yield_raw = pd.read_csv(YIELD_PATH, sep=\";\")\n",
    "df_climate_raw = pd.read_parquet(CLIMATE_PATH)\n",
    "\n",
    "print(\"Yield raw shape:\", df_yield_raw.shape)\n",
    "print(\"Climate raw shape:\", df_climate_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean column names.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with standardized column names.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "        .str.replace(r\"[^a-z0-9_]\", \"\", regex=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df_yield = clean_cols(df_yield_raw)\n",
    "df_yield = df_yield.rename(columns={\"department\": \"nom_dep\"})\n",
    "df_yield[\"year\"] = pd.to_numeric(df_yield[\"year\"], errors=\"coerce\")\n",
    "df_yield[\"yield\"] = pd.to_numeric(df_yield[\"yield\"], errors=\"coerce\")\n",
    "df_yield[\"area\"] = pd.to_numeric(df_yield[\"area\"], errors=\"coerce\")\n",
    "df_yield[\"production\"] = pd.to_numeric(df_yield[\"production\"], errors=\"coerce\")\n",
    "\n",
    "df_climate = clean_cols(df_climate_raw)\n",
    "df_climate[\"time\"] = pd.to_datetime(df_climate[\"time\"])\n",
    "\n",
    "print(\"Yield columns:\", df_yield.columns.tolist())\n",
    "print(\"Climate columns:\", df_climate.columns.tolist())\n",
    "display(df_yield.head())\n",
    "display(df_climate.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Yield Table — Deep Missing Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Overall missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"YIELD TABLE — MISSING VALUES PER COLUMN\")\n",
    "print(\"=\" * 60)\n",
    "for col in df_yield.columns:\n",
    "    n_miss = df_yield[col].isna().sum()\n",
    "    pct = 100 * n_miss / len(df_yield)\n",
    "    print(f\"  {col:15s}: {n_miss:4d} missing  ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal rows: {len(df_yield)}\")\n",
    "print(f\"Rows with ANY missing: {df_yield.isna().any(axis=1).sum()}\")\n",
    "print(\n",
    "    f\"Rows with ALL of yield/area/production missing: \"\n",
    "    f\"{df_yield[['yield', 'area', 'production']].isna().all(axis=1).sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing values overlap (yield vs area vs production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_yield = df_yield[\"yield\"].isna()\n",
    "miss_area = df_yield[\"area\"].isna()\n",
    "miss_prod = df_yield[\"production\"].isna()\n",
    "\n",
    "print(\"OVERLAP OF MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "print(\n",
    "    \"  yield NaN only (area & prod present) : \"\n",
    "    f\"{(miss_yield & ~miss_area & ~miss_prod).sum()}\"\n",
    ")\n",
    "print(\n",
    "    \"  yield NaN + area NaN + prod NaN      : \"\n",
    "    f\"{(miss_yield & miss_area & miss_prod).sum()}\"\n",
    ")\n",
    "print(\n",
    "    \"  yield NaN + area NaN (prod present)  : \"\n",
    "    f\"{(miss_yield & miss_area & ~miss_prod).sum()}\"\n",
    ")\n",
    "print(\n",
    "    f\"  yield NaN + prod NaN (area present)  : \"\n",
    "    f\"{(miss_yield & ~miss_area & miss_prod).sum()}\"\n",
    ")\n",
    "print(f\"  area NaN only (yield present)        : {(~miss_yield & miss_area).sum()}\")\n",
    "print(f\"  production NaN only (yield present)  : {(~miss_yield & miss_prod).sum()}\")\n",
    "\n",
    "# Crosstab for clarity\n",
    "df_yield[\"yield_miss\"] = miss_yield.map({True: \"yield NaN\", False: \"yield OK\"})\n",
    "df_yield[\"area_miss\"] = miss_area.map({True: \"area NaN\", False: \"area OK\"})\n",
    "df_yield[\"prod_miss\"] = miss_prod.map({True: \"prod NaN\", False: \"prod OK\"})\n",
    "\n",
    "print(\"\\nCrosstab: yield_miss vs area_miss\")\n",
    "display(pd.crosstab(df_yield[\"yield_miss\"], df_yield[\"area_miss\"], margins=True))\n",
    "print(\"\\nCrosstab: yield_miss vs prod_miss\")\n",
    "display(pd.crosstab(df_yield[\"yield_miss\"], df_yield[\"prod_miss\"], margins=True))\n",
    "\n",
    "# Clean up temp columns\n",
    "df_yield.drop(columns=[\"yield_miss\", \"area_miss\", \"prod_miss\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Recoverability: can we compute yield = production / area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoverable = miss_yield & ~miss_area & ~miss_prod & (df_yield[\"area\"] > 0)\n",
    "not_recoverable = miss_yield & ~recoverable\n",
    "\n",
    "print(\"YIELD RECOVERABILITY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total missing yield          : {miss_yield.sum()}\")\n",
    "print(f\"  Recoverable (prod/area)      : {recoverable.sum()}\")\n",
    "print(f\"  NOT recoverable              : {not_recoverable.sum()}\")\n",
    "\n",
    "# Show the recoverable rows to verify they look reasonable\n",
    "if recoverable.sum() > 0:\n",
    "    sample = df_yield[recoverable].copy()\n",
    "    sample[\"yield_computed\"] = sample[\"production\"] / sample[\"area\"]\n",
    "    print(\"\\nSample of recoverable rows (computed yield = production / area):\")\n",
    "    display(\n",
    "        sample[\n",
    "            [\"nom_dep\", \"year\", \"yield\", \"area\", \"production\", \"yield_computed\"]\n",
    "        ].head(15)\n",
    "    )\n",
    "    print(\"\\nComputed yield stats:\")\n",
    "    display(sample[\"yield_computed\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Consistency check: yield vs production/area (when all three present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When yield, area, production are all present, does yield ~= production/area?\n",
    "mask_all_present = ~miss_yield & ~miss_area & ~miss_prod & (df_yield[\"area\"] > 0)\n",
    "check = df_yield[mask_all_present].copy()\n",
    "check[\"yield_computed\"] = check[\"production\"] / check[\"area\"]\n",
    "check[\"diff\"] = (check[\"yield\"] - check[\"yield_computed\"]).abs()\n",
    "check[\"diff_pct\"] = 100 * check[\"diff\"] / check[\"yield\"].replace(0, np.nan)\n",
    "\n",
    "print(\"CONSISTENCY: yield vs production/area\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Rows checked: {len(check)}\")\n",
    "print(\"\\nAbsolute difference stats:\")\n",
    "display(check[\"diff\"].describe())\n",
    "print(f\"\\nRows with >1% discrepancy: {(check['diff_pct'] > 1).sum()}\")\n",
    "print(f\"Rows with >5% discrepancy: {(check['diff_pct'] > 5).sum()}\")\n",
    "print(f\"Rows with >10% discrepancy: {(check['diff_pct'] > 10).sum()}\")\n",
    "\n",
    "if (check[\"diff_pct\"] > 5).sum() > 0:\n",
    "    print(\"\\nRows with >5% discrepancy:\")\n",
    "    display(\n",
    "        check[check[\"diff_pct\"] > 5][\n",
    "            [\n",
    "                \"nom_dep\",\n",
    "                \"year\",\n",
    "                \"yield\",\n",
    "                \"area\",\n",
    "                \"production\",\n",
    "                \"yield_computed\",\n",
    "                \"diff_pct\",\n",
    "            ]\n",
    "        ]\n",
    "        .sort_values(\"diff_pct\", ascending=False)\n",
    "        .head(20)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Missing yield by YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing yield by year\n",
    "year_stats = (\n",
    "    df_yield.groupby(\"year\")\n",
    "    .agg(\n",
    "        total_depts=(\"nom_dep\", \"count\"),\n",
    "        yield_missing=(\"yield\", lambda x: x.isna().sum()),\n",
    "        area_missing=(\"area\", lambda x: x.isna().sum()),\n",
    "        prod_missing=(\"production\", lambda x: x.isna().sum()),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "year_stats[\"yield_miss_pct\"] = (\n",
    "    100 * year_stats[\"yield_missing\"] / year_stats[\"total_depts\"]\n",
    ").round(1)\n",
    "\n",
    "print(\"MISSING BY YEAR\")\n",
    "print(\"=\" * 60)\n",
    "display(year_stats)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.bar(\n",
    "    year_stats[\"year\"],\n",
    "    year_stats[\"yield_missing\"],\n",
    "    color=\"coral\",\n",
    "    label=\"yield missing\",\n",
    ")\n",
    "ax.bar(\n",
    "    year_stats[\"year\"],\n",
    "    year_stats[\"area_missing\"],\n",
    "    color=\"steelblue\",\n",
    "    alpha=0.7,\n",
    "    label=\"area missing\",\n",
    ")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Number of departments with missing data\")\n",
    "ax.set_title(\"Missing data per year — Yield table\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Missing yield by DEPARTMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_stats = (\n",
    "    df_yield.groupby(\"nom_dep\")\n",
    "    .agg(\n",
    "        total_years=(\"year\", \"count\"),\n",
    "        yield_missing=(\"yield\", lambda x: x.isna().sum()),\n",
    "        area_missing=(\"area\", lambda x: x.isna().sum()),\n",
    "        prod_missing=(\"production\", lambda x: x.isna().sum()),\n",
    "        yield_mean=(\"yield\", \"mean\"),\n",
    "        area_mean=(\"area\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "dep_stats[\"yield_miss_pct\"] = (\n",
    "    100 * dep_stats[\"yield_missing\"] / dep_stats[\"total_years\"]\n",
    ").round(1)\n",
    "dep_stats = dep_stats.sort_values(\"yield_missing\", ascending=False)\n",
    "\n",
    "print(\"MISSING BY DEPARTMENT (sorted by yield_missing)\")\n",
    "print(\"=\" * 60)\n",
    "display(dep_stats.head(30))\n",
    "\n",
    "# Departments with >20% missing yield\n",
    "high_miss = dep_stats[dep_stats[\"yield_miss_pct\"] > 20]\n",
    "print(f\"\\nDepartments with >20% missing yield: {len(high_miss)}\")\n",
    "display(\n",
    "    high_miss[\n",
    "        [\n",
    "            \"nom_dep\",\n",
    "            \"total_years\",\n",
    "            \"yield_missing\",\n",
    "            \"yield_miss_pct\",\n",
    "            \"yield_mean\",\n",
    "            \"area_mean\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: top 30 departments by missing yield count\n",
    "top30 = dep_stats.head(30)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(top30[\"nom_dep\"], top30[\"yield_missing\"], color=\"coral\")\n",
    "ax.set_xlabel(\"Number of missing yield values\")\n",
    "ax.set_title(\"Top 30 departments by missing yield count\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Departments with ZERO yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Departments where yield is ALWAYS missing\n",
    "zero_yield_depts = dep_stats[dep_stats[\"yield_missing\"] == dep_stats[\"total_years\"]]\n",
    "print(f\"Departments with ALL yield missing: {len(zero_yield_depts)}\")\n",
    "if len(zero_yield_depts) > 0:\n",
    "    display(zero_yield_depts[[\"nom_dep\", \"total_years\", \"yield_missing\", \"area_mean\"]])\n",
    "\n",
    "# Departments where yield is NEVER missing\n",
    "full_yield_depts = dep_stats[dep_stats[\"yield_missing\"] == 0]\n",
    "print(f\"\\nDepartments with NO missing yield: {len(full_yield_depts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Yield outlier check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_yield = df_yield.dropna(subset=[\"yield\"])\n",
    "\n",
    "print(\"YIELD VALUE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "display(valid_yield[\"yield\"].describe())\n",
    "\n",
    "# IQR method for outliers\n",
    "q1, q3 = valid_yield[\"yield\"].quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "low = q1 - 1.5 * iqr\n",
    "high = q3 + 1.5 * iqr\n",
    "outliers = valid_yield[(valid_yield[\"yield\"] < low) | (valid_yield[\"yield\"] > high)]\n",
    "\n",
    "print(f\"\\nIQR bounds: [{low:.2f}, {high:.2f}]\")\n",
    "print(\n",
    "    f\"Outliers (IQR method): \"\n",
    "    f\"{len(outliers)} ({100 * len(outliers) / len(valid_yield):.1f}%)\"\n",
    ")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(\"\\nOutlier rows:\")\n",
    "    display(\n",
    "        outliers[[\"nom_dep\", \"year\", \"yield\", \"area\", \"production\"]].sort_values(\n",
    "            \"yield\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "axes[0].hist(valid_yield[\"yield\"], bins=50, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0].axvline(low, color=\"red\", ls=\"--\", label=f\"IQR low={low:.1f}\")\n",
    "axes[0].axvline(high, color=\"red\", ls=\"--\", label=f\"IQR high={high:.1f}\")\n",
    "axes[0].set_title(\"Yield distribution\")\n",
    "axes[0].legend()\n",
    "axes[1].boxplot(valid_yield[\"yield\"], vert=True)\n",
    "axes[1].set_title(\"Yield boxplot\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Area = 0 or very small (suspect rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows where area = 0 or production = 0\n",
    "zero_area = df_yield[df_yield[\"area\"] == 0]\n",
    "zero_prod = df_yield[df_yield[\"production\"] == 0]\n",
    "\n",
    "print(f\"Rows with area = 0: {len(zero_area)}\")\n",
    "if len(zero_area) > 0:\n",
    "    display(zero_area[[\"nom_dep\", \"year\", \"yield\", \"area\", \"production\"]])\n",
    "\n",
    "print(f\"\\nRows with production = 0: {len(zero_prod)}\")\n",
    "if len(zero_prod) > 0:\n",
    "    display(zero_prod[[\"nom_dep\", \"year\", \"yield\", \"area\", \"production\"]].head(20))\n",
    "\n",
    "# Very small areas\n",
    "small_area = df_yield[(df_yield[\"area\"] > 0) & (df_yield[\"area\"] < 100)]\n",
    "print(f\"\\nRows with 0 < area < 100 (very small): {len(small_area)}\")\n",
    "if len(small_area) > 0:\n",
    "    display(small_area[[\"nom_dep\", \"year\", \"yield\", \"area\", \"production\"]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Heatmap: missing yield by (department x year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot: department x year, value = 1 if yield present, 0 if missing\n",
    "pivot = df_yield.pivot_table(\n",
    "    index=\"nom_dep\", columns=\"year\", values=\"yield\", aggfunc=lambda x: x.isna().sum()\n",
    ").fillna(0)\n",
    "\n",
    "# Only show departments that have at least 1 missing\n",
    "pivot_miss = pivot[pivot.sum(axis=1) > 0].sort_values(\n",
    "    by=pivot.columns.tolist(), ascending=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, max(6, len(pivot_miss) * 0.3)))\n",
    "sns.heatmap(\n",
    "    pivot_miss,\n",
    "    cmap=\"YlOrRd\",\n",
    "    linewidths=0.3,\n",
    "    linecolor=\"grey\",\n",
    "    cbar_kws={\"label\": \"Missing yield count\"},\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Missing yield heatmap: department x year\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Department\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Climate Table — Deep Missing Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Overall missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CLIMATE TABLE — MISSING VALUES PER COLUMN\")\n",
    "print(\"=\" * 60)\n",
    "for col in df_climate.columns:\n",
    "    n_miss = df_climate[col].isna().sum()\n",
    "    pct = 100 * n_miss / len(df_climate)\n",
    "    print(f\"  {col:15s}: {n_miss:8d} missing  ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal rows: {len(df_climate):,}\")\n",
    "print(f\"Rows with ANY missing: {df_climate.isna().any(axis=1).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scenario & metric breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COVERAGE BY SCENARIO\")\n",
    "print(\"=\" * 60)\n",
    "for sc in sorted(df_climate[\"scenario\"].unique()):\n",
    "    sub = df_climate[df_climate[\"scenario\"] == sc]\n",
    "    y_min, y_max = sub[\"year\"].min(), sub[\"year\"].max()\n",
    "    n_deps = sub[\"nom_dep\"].nunique()\n",
    "    n_metrics = sub[\"metric\"].nunique()\n",
    "    print(\n",
    "        f\"  {sc:15s}: years {y_min}-{y_max} | \"\n",
    "        f\"{n_deps:3d} departments | {n_metrics} metrics | {len(sub):,} rows\"\n",
    "    )\n",
    "\n",
    "print(\"\\nMETRICS\")\n",
    "print(\"=\" * 60)\n",
    "for m in df_climate[\"metric\"].unique():\n",
    "    n = (df_climate[\"metric\"] == m).sum()\n",
    "    print(f\"  {m}: {n:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Focus on HISTORICAL scenario (training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_hist = df_climate[df_climate[\"scenario\"] == \"historical\"].copy()\n",
    "print(f\"Historical climate data: {len(clim_hist):,} rows\")\n",
    "print(f\"Departments: {clim_hist['nom_dep'].nunique()}\")\n",
    "print(f\"Year range: {clim_hist['year'].min()} - {clim_hist['year'].max()}\")\n",
    "print(f\"Metrics: {clim_hist['metric'].unique().tolist()}\")\n",
    "\n",
    "# Check completeness: expected days per (department, year, metric)\n",
    "days_per_group = (\n",
    "    clim_hist.groupby([\"nom_dep\", \"year\", \"metric\"]).size().reset_index(name=\"n_days\")\n",
    ")\n",
    "print(\"\\nExpected ~365 days per (dept, year, metric)\")\n",
    "display(days_per_group[\"n_days\"].describe())\n",
    "\n",
    "incomplete = days_per_group[days_per_group[\"n_days\"] < 365]\n",
    "print(f\"\\nIncomplete groups (< 365 days): {len(incomplete)}\")\n",
    "if len(incomplete) > 0:\n",
    "    display(incomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Climate departments list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_depts = sorted(clim_hist[\"nom_dep\"].unique())\n",
    "print(f\"Historical climate departments ({len(clim_depts)}):\")\n",
    "for i, d in enumerate(clim_depts, 1):\n",
    "    print(f\"  {i:3d}. {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Coverage per department x year (historical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For historical: which (department, year) combos exist?\n",
    "hist_coverage = clim_hist.groupby([\"nom_dep\", \"year\"]).size().reset_index(name=\"n_rows\")\n",
    "all_years = list(range(1982, 2015))  # historical: 1982-2014\n",
    "all_combos = pd.MultiIndex.from_product(\n",
    "    [clim_depts, all_years], names=[\"nom_dep\", \"year\"]\n",
    ")\n",
    "coverage_full = pd.DataFrame(index=all_combos).reset_index()\n",
    "coverage_full = coverage_full.merge(hist_coverage, on=[\"nom_dep\", \"year\"], how=\"left\")\n",
    "coverage_full[\"has_data\"] = coverage_full[\"n_rows\"].notna()\n",
    "\n",
    "missing_combos = coverage_full[~coverage_full[\"has_data\"]]\n",
    "print(f\"Missing (dept, year) combinations in historical climate: {len(missing_combos)}\")\n",
    "if len(missing_combos) > 0:\n",
    "    print(\"\\nMissing combos:\")\n",
    "    display(missing_combos[[\"nom_dep\", \"year\"]].head(50))\n",
    "\n",
    "    # By department\n",
    "    miss_by_dep = missing_combos.groupby(\"nom_dep\").size().sort_values(ascending=False)\n",
    "    print(\"\\nMissing years per department:\")\n",
    "    display(miss_by_dep.to_frame(\"n_missing_years\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Data quality: negative precipitation & value ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA QUALITY BY METRIC (historical only)\")\n",
    "print(\"=\" * 60)\n",
    "for m in clim_hist[\"metric\"].unique():\n",
    "    vals = clim_hist[clim_hist[\"metric\"] == m][\"value\"]\n",
    "    n_neg = (vals < 0).sum()\n",
    "    n_inf = np.isinf(vals).sum()\n",
    "    n_nan = vals.isna().sum()\n",
    "    print(f\"\\n  {m}:\")\n",
    "    print(f\"    Count: {len(vals):,}\")\n",
    "    print(f\"    Range: [{vals.min():.6f}, {vals.max():.6f}]\")\n",
    "    print(f\"    Mean: {vals.mean():.6f}, Std: {vals.std():.6f}\")\n",
    "    print(\n",
    "        f\"    NaN: {n_nan:,}  |  Inf: {n_inf:,}  |  \"\n",
    "        f\"Negative: {n_neg:,} ({100 * n_neg / len(vals):.2f}%)\"\n",
    "    )\n",
    "    if n_neg > 0:\n",
    "        print(f\"    Min negative: {vals[vals < 0].min():.2e}\")\n",
    "        print(f\"    Max negative: {vals[vals < 0].max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 SSP scenarios: departments & coverage differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if SSP scenarios have the same departments as historical\n",
    "print(\"DEPARTMENT COVERAGE ACROSS SCENARIOS\")\n",
    "print(\"=\" * 60)\n",
    "scenario_depts = {}\n",
    "for sc in sorted(df_climate[\"scenario\"].unique()):\n",
    "    depts = set(df_climate[df_climate[\"scenario\"] == sc][\"nom_dep\"].unique())\n",
    "    scenario_depts[sc] = depts\n",
    "    print(f\"  {sc:15s}: {len(depts)} departments\")\n",
    "\n",
    "# Cross-compare\n",
    "hist_d = scenario_depts[\"historical\"]\n",
    "for sc in [\"ssp1_2_6\", \"ssp2_4_5\", \"ssp5_8_5\"]:\n",
    "    diff_from_hist = hist_d - scenario_depts[sc]\n",
    "    diff_from_ssp = scenario_depts[sc] - hist_d\n",
    "    print(\n",
    "        f\"\\n  In historical but NOT in {sc}: \"\n",
    "        f\"{sorted(diff_from_hist) if diff_from_hist else 'none'}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  In {sc} but NOT in historical: \"\n",
    "        f\"{sorted(diff_from_ssp) if diff_from_ssp else 'none'}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Department Mismatch — Detailed Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Side-by-side comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depts_yield = set(df_yield[\"nom_dep\"].dropna().str.strip().unique())\n",
    "depts_climate = set(clim_hist[\"nom_dep\"].dropna().str.strip().unique())\n",
    "\n",
    "in_both = sorted(depts_yield & depts_climate)\n",
    "in_yield_only = sorted(depts_yield - depts_climate)\n",
    "in_climate_only = sorted(depts_climate - depts_yield)\n",
    "\n",
    "print(\"DEPARTMENT COMPARISON: YIELD vs CLIMATE (historical)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Yield departments     : {len(depts_yield)}\")\n",
    "print(f\"  Climate departments   : {len(depts_climate)}\")\n",
    "print(f\"  In BOTH (can join)    : {len(in_both)}\")\n",
    "print(f\"  In YIELD only         : {len(in_yield_only)}\")\n",
    "print(f\"  In CLIMATE only       : {len(in_climate_only)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_yield_only:\n",
    "    print(\"DEPARTMENTS IN YIELD ONLY (no climate data):\")\n",
    "    print(\"=\" * 60)\n",
    "    for d in in_yield_only:\n",
    "        sub = df_yield[df_yield[\"nom_dep\"].str.strip() == d]\n",
    "        n_total = len(sub)\n",
    "        n_yield_ok = sub[\"yield\"].notna().sum()\n",
    "        mean_yield = sub[\"yield\"].mean()\n",
    "        mean_area = sub[\"area\"].mean()\n",
    "        print(f\"\\n  {d}:\")\n",
    "        print(\n",
    "            f\"    Rows: {n_total} | Yield present: {n_yield_ok} | \"\n",
    "            f\"Mean yield: {mean_yield:.2f} | Mean area: {mean_area:.0f}\"\n",
    "        )\n",
    "        print(f\"    Years: {sorted(sub['year'].tolist())}\")\n",
    "\n",
    "if in_climate_only:\n",
    "    print(\"\\nDEPARTMENTS IN CLIMATE ONLY (no yield data):\")\n",
    "    print(\"=\" * 60)\n",
    "    for d in in_climate_only:\n",
    "        sub = clim_hist[clim_hist[\"nom_dep\"].str.strip() == d]\n",
    "        print(f\"  {d}: {sub['year'].nunique()} years of climate data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fuzzy matching: are the mismatches naming issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fuzzy check: normalize and compare\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize the departmentnames.\n",
    "\n",
    "    Args:\n",
    "        name (str): The department name to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized department name.\n",
    "    \"\"\"\n",
    "    return name.lower().replace(\"_\", \" \").replace(\"-\", \" \").replace(\"'\", \" \").strip()\n",
    "\n",
    "\n",
    "print(\"FUZZY MATCHING ATTEMPT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "yield_norm = {normalize_name(d): d for d in depts_yield}\n",
    "climate_norm = {normalize_name(d): d for d in depts_climate}\n",
    "\n",
    "# Check if any yield-only dept normalizes to a climate dept\n",
    "found_match = False\n",
    "for d in in_yield_only:\n",
    "    d_norm = normalize_name(d)\n",
    "    if d_norm in climate_norm:\n",
    "        print(\n",
    "            f\"  MATCH: yield '{d}' -> climate '{climate_norm[d_norm]}' \"\n",
    "            \"(same after normalization)\"\n",
    "        )\n",
    "        found_match = True\n",
    "    else:\n",
    "        # Check substring matches\n",
    "        for cn, co in climate_norm.items():\n",
    "            if d_norm in cn or cn in d_norm:\n",
    "                print(f\"  SIMILAR: yield '{d}' ~ climate '{co}'\")\n",
    "                found_match = True\n",
    "\n",
    "if not found_match:\n",
    "    print(\"  No fuzzy matches found — these are genuinely different departments.\")\n",
    "\n",
    "# Also list full sorted dept names side by side\n",
    "print(\"\\n\\nFULL DEPARTMENT LISTS\")\n",
    "print(\"=\" * 60)\n",
    "all_depts = sorted(depts_yield | depts_climate)\n",
    "print(f\"{'Department':<30s} {'In Yield':>10s} {'In Climate':>10s}\")\n",
    "print(\"-\" * 55)\n",
    "for d in all_depts:\n",
    "    in_y = \"Yes\" if d in depts_yield else \"-\"\n",
    "    in_c = \"Yes\" if d in depts_climate else \"-\"\n",
    "    marker = \" ***\" if in_y != in_c else \"\"\n",
    "    print(f\"  {d:<28s} {in_y:>10s} {in_c:>10s}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Impact quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many yield rows are affected by the department mismatch?\n",
    "yield_no_climate = df_yield[df_yield[\"nom_dep\"].str.strip().isin(in_yield_only)]\n",
    "yield_has_climate = df_yield[df_yield[\"nom_dep\"].str.strip().isin(in_both)]\n",
    "\n",
    "print(\"IMPACT OF DEPARTMENT MISMATCH ON YIELD TABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(\n",
    "    f\"  Yield rows with climate match    : {len(yield_has_climate)} \"\n",
    "    f\"({100 * len(yield_has_climate) / len(df_yield):.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Yield rows WITHOUT climate match : {len(yield_no_climate)} \"\n",
    "    f\"({100 * len(yield_no_climate) / len(df_yield):.1f}%)\"\n",
    ")\n",
    "print(\"\\n  Of those without climate:\")\n",
    "print(f\"    With valid yield  : {yield_no_climate['yield'].notna().sum()}\")\n",
    "print(f\"    With missing yield: {yield_no_climate['yield'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cross-table Join Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Year coverage mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_years = set(df_yield[\"year\"].dropna().unique())\n",
    "climate_hist_years = set(clim_hist[\"year\"].unique())\n",
    "\n",
    "print(\"YEAR COVERAGE\")\n",
    "print(\"=\" * 60)\n",
    "print(\n",
    "    f\"  Yield years   : {min(yield_years):.0f} - {max(yield_years):.0f} \"\n",
    "    f\"({len(yield_years)} years)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Climate years : {min(climate_hist_years)} - {max(climate_hist_years)} \"\n",
    "    f\"({len(climate_hist_years)} years)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Overlap       : {min(yield_years & climate_hist_years):.0f} - \"\n",
    "    f\"{max(yield_years & climate_hist_years):.0f} \"\n",
    "    f\"({len(yield_years & climate_hist_years)} years)\"\n",
    ")\n",
    "\n",
    "yield_no_climate_years = sorted(yield_years - climate_hist_years)\n",
    "climate_no_yield_years = sorted(climate_hist_years - yield_years)\n",
    "\n",
    "print(\n",
    "    \"\\n  Yield years WITHOUT historical climate: \"\n",
    "    f\"{yield_no_climate_years if yield_no_climate_years else 'none'}\"\n",
    ")\n",
    "print(\n",
    "    \"  Climate years WITHOUT yield           : \"\n",
    "    f\"{climate_no_yield_years if climate_no_yield_years else 'none'}\"\n",
    ")\n",
    "\n",
    "if yield_no_climate_years:\n",
    "    n_affected = df_yield[df_yield[\"year\"].isin(yield_no_climate_years)].shape[0]\n",
    "    print(f\"\\n  Yield rows in years without historical climate: {n_affected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Simulated join: what NaN would we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate climate to (nom_dep, year) level — simple mean per metric\n",
    "clim_agg = (\n",
    "    clim_hist.groupby([\"nom_dep\", \"year\", \"metric\"])[\"value\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot_table(index=[\"nom_dep\", \"year\"], columns=\"metric\", values=\"value\")\n",
    "    .reset_index()\n",
    ")\n",
    "clim_agg.columns.name = None\n",
    "\n",
    "# Join\n",
    "merged = df_yield.merge(clim_agg, on=[\"nom_dep\", \"year\"], how=\"left\")\n",
    "\n",
    "print(\"SIMULATED JOIN: yield LEFT JOIN climate_agg\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Merged shape: {merged.shape}\")\n",
    "print(\"\\n  Missing values after join:\")\n",
    "for col in merged.columns:\n",
    "    n = merged[col].isna().sum()\n",
    "    pct = 100 * n / len(merged)\n",
    "    if n > 0:\n",
    "        print(f\"    {col:45s}: {n:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose WHY climate columns are NaN\n",
    "climate_cols = [c for c in merged.columns if c not in df_yield.columns]\n",
    "any_climate_nan = merged[climate_cols].isna().any(axis=1)\n",
    "\n",
    "nan_rows = merged[any_climate_nan].copy()\n",
    "nan_rows[\"reason\"] = \"unknown\"\n",
    "nan_rows.loc[nan_rows[\"nom_dep\"].str.strip().isin(in_yield_only), \"reason\"] = (\n",
    "    \"dept_not_in_climate\"\n",
    ")\n",
    "nan_rows.loc[\n",
    "    (nan_rows[\"reason\"] == \"unknown\") & (~nan_rows[\"year\"].isin(climate_hist_years)),\n",
    "    \"reason\",\n",
    "] = \"year_not_in_climate\"\n",
    "\n",
    "print(\"REASONS FOR NaN CLIMATE FEATURES AFTER JOIN\")\n",
    "print(\"=\" * 60)\n",
    "display(nan_rows[\"reason\"].value_counts().to_frame(\"count\"))\n",
    "\n",
    "if (nan_rows[\"reason\"] == \"unknown\").sum() > 0:\n",
    "    print(\"\\nRows with unknown reason for NaN:\")\n",
    "    display(\n",
    "        nan_rows[nan_rows[\"reason\"] == \"unknown\"][\n",
    "            [\"nom_dep\", \"year\"] + climate_cols\n",
    "        ].head(20)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Summary: rows usable for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_yield = merged[\"yield\"].notna()\n",
    "has_climate = merged[climate_cols].notna().all(axis=1)\n",
    "\n",
    "print(\"USABLE ROWS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total yield rows             : {len(merged)}\")\n",
    "print(\n",
    "    f\"  With yield present           : \"\n",
    "    f\"{has_yield.sum()} ({100 * has_yield.mean():.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  With climate present         : \"\n",
    "    f\"{has_climate.sum()} ({100 * has_climate.mean():.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  With BOTH yield AND climate  : {(has_yield & has_climate).sum()} \"\n",
    "    f\"({100 * (has_yield & has_climate).mean():.1f}%)\"\n",
    ")\n",
    "print(f\"\\n  Lost due to missing yield   : {(~has_yield).sum()}\")\n",
    "print(f\"  Lost due to missing climate  : {(has_yield & ~has_climate).sum()}\")\n",
    "print(\n",
    "    f\"  Lost total (from original)   : {len(merged) - (has_yield & has_climate).sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary of All Issues — Decision Points\n",
    "\n",
    "Below is a consolidated summary of every data quality issue found. We can discuss each one and decide what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    SUMMARY OF ALL DATA QUALITY ISSUES                              ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "YIELD TABLE\n",
    "──────────────────────────────────────────────────────────────────────────────────────\n",
    "Issue #1: Missing yield values\n",
    "  - 247 rows (~6.9%) have NaN yield\n",
    "  - ~125 can be RECOVERED via yield = production / area\n",
    "  - ~122 cannot be recovered (area and/or production also missing)\n",
    "  → DECISION: Recover where possible? Drop the rest?\n",
    "\n",
    "Issue #2: Missing area values\n",
    "  - 115 rows (~3.2%) have NaN area\n",
    "  - Most overlap with missing yield\n",
    "  → DECISION: Drop? Or keep rows where yield is present even if area is NaN?\n",
    "\n",
    "Issue #3: Missing production values\n",
    "  - 122 rows (~3.4%) have NaN production\n",
    "  → DECISION: Same as area — keep if yield is present?\n",
    "\n",
    "Issue #4: Departments with extremely high missing rates\n",
    "  - Paris, Hauts-de-Seine, Seine-Saint-Denis, Val-de-Marne, Val-d'Oise,\n",
    "    Seine-et-Marne (urban Île-de-France) → minimal barley production\n",
    "  → DECISION: Drop these departments entirely?\n",
    "\n",
    "Issue #5: Area = 0 or very small areas\n",
    "  - Some rows have area = 0 (no barley cultivation)\n",
    "  → DECISION: Drop rows with area = 0?\n",
    "\n",
    "Issue #6: Yield outliers\n",
    "  - Some extreme low values (< 1 t/ha)\n",
    "  → DECISION: Keep, clip, or investigate?\n",
    "\n",
    "CLIMATE TABLE\n",
    "──────────────────────────────────────────────────────────────────────────────────────\n",
    "Issue #7: No raw NaN values\n",
    "  - Climate data has 0% missing → good!\n",
    "  → No action needed on raw missing.\n",
    "\n",
    "Issue #8: Negative precipitation values\n",
    "  - ~4% of precipitation rows have tiny negative values (~1e-25)\n",
    "  - These are numerical noise, not real\n",
    "  → DECISION: Clip to 0?\n",
    "\n",
    "Issue #9: Temperature in Kelvin\n",
    "  - Values are in Kelvin (not Celsius)\n",
    "  → DECISION: Convert to Celsius (subtract 273.15)? Or keep in K?\n",
    "\n",
    "CROSS-TABLE ISSUES\n",
    "──────────────────────────────────────────────────────────────────────────────────────\n",
    "Issue #10: Department mismatch\n",
    "  - 8 departments in yield have NO climate data\n",
    "  - These are: Corse_du_Sud, Haute_Corse, Hauts_de_Seine, Paris,\n",
    "    Seine_Saint_Denis, Seine_SeineOise, Val_d_Oise, Val_de_Marne\n",
    "  - Affects ~290 yield rows (8.1%)\n",
    "  - No fuzzy name matches → genuinely missing, not a naming issue\n",
    "  → DECISION: Drop yield rows for these departments?\n",
    "\n",
    "Issue #11: Year coverage mismatch\n",
    "  - Yield: 1982-2018, Climate historical: 1982-2014\n",
    "  - Yield rows for 2015-2018 have no historical climate data\n",
    "  - SSP scenarios start at 2015 (future projections, not observations)\n",
    "  → DECISION: Drop yield rows 2015-2018? Or use SSP data as proxy?\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final quantification: how many rows would survive each cleaning scenario\n",
    "total = len(df_yield)\n",
    "\n",
    "# Scenario A: minimal cleaning (just drop rows where yield is NaN after recovery)\n",
    "df_a = df_yield.copy()\n",
    "recover_mask = (\n",
    "    df_a[\"yield\"].isna()\n",
    "    & df_a[\"area\"].notna()\n",
    "    & df_a[\"production\"].notna()\n",
    "    & (df_a[\"area\"] > 0)\n",
    ")\n",
    "df_a.loc[recover_mask, \"yield\"] = (\n",
    "    df_a.loc[recover_mask, \"production\"] / df_a.loc[recover_mask, \"area\"]\n",
    ")\n",
    "a_after_recovery = df_a[\"yield\"].notna().sum()\n",
    "a_lost = total - a_after_recovery\n",
    "\n",
    "# Scenario B: + drop departments not in climate\n",
    "df_b = df_a[df_a[\"nom_dep\"].str.strip().isin(in_both)]\n",
    "b_after = df_b[\"yield\"].notna().sum()\n",
    "\n",
    "# Scenario C: + drop years not in historical climate (2015-2018)\n",
    "df_c = df_b[df_b[\"year\"].isin(climate_hist_years)]\n",
    "c_after = df_c[\"yield\"].notna().sum()\n",
    "\n",
    "# Scenario D: + drop area = 0\n",
    "df_d = df_c[df_c[\"area\"] > 0]\n",
    "d_after = df_d[\"yield\"].notna().sum()\n",
    "\n",
    "print(\"CLEANING SCENARIOS — ROW SURVIVAL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Original                                  : {total:5d} rows\")\n",
    "print(\n",
    "    f\"  A) Recover yield + drop NaN yield         : {a_after_recovery:5d} rows \"\n",
    "    f\"(lost {a_lost})\"\n",
    ")\n",
    "print(f\"  B) + Drop depts not in climate            : {b_after:5d} rows\")\n",
    "print(f\"  C) + Drop years 2015-2018 (no hist clim)  : {c_after:5d} rows\")\n",
    "print(f\"  D) + Drop area = 0                        : {d_after:5d} rows\")\n",
    "print(f\"\\n  Final usable rows: {d_after} out of {total} ({100 * d_after / total:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
